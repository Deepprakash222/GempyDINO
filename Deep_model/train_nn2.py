import torch
import torch.nn as nn
import numpy as np

import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader, random_split

dtype = torch.float32


    
def build_network(layer_sizes):
    """
    This function is used to generate a fully connected neural network

    Args:
        layer_sizes (list of int): A list of integer representing the number of nodes in each layer

    Returns:
        torch.nn.Sequential: A sequential neural network model composed of linear layers and
        ReLU activations (except for the output layer).
    """
    layers = []
    for i in range(len(layer_sizes) - 1):
        layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))  # Fully connected layer
        if i < len(layer_sizes) - 2:  # No activation for last layer
            #layers.append(nn.ReLU())  # Activation function (can be modified)
            layers.append(nn.Tanh())
            
    
    return nn.Sequential(*layers)

class NeuralNet(nn.Module):
    """
    A fully connected feedforward neural network (multi-layer perceptron).

    Args:
        nn (class): Base class of neural network. It is a parent class of the current class.
    """
    
    def __init__(self, layer_sizes):
        """
        Constructor Method for the NeuralNet class

        Args:
            layer_sizes (list of int): A list of integer representing the number of nodes in each layers.
        """
        super(NeuralNet, self).__init__() # Initialialise the parent class
        self.network = build_network(layer_sizes)  # Use the dynamic builder

    def forward(self, x):
        """
        It is used to represet the neural network. 

        Args:
            x (float): Input to the neural network

        Returns:
            float: Output of the neural network
        """
        return self.network(x)
    
def M_norm(u, v, M):
    u_T_M = torch.matmul(u,M)
    u_T_M_v = torch.matmul(u_T_M, v)
    
    return u_T_M_v

def L2_accuracy(true_Data, nueral_network_output, M):
    """
    This function is used to calculate the L2 norm accuracy. 
    L2_accuracy = 1 - sqrt(E[||true_data - neural_networ_output||_2^2/||true_data||_2^2])

    Args:
        true_Data (float): True output data
        nueral_network_output (float): Output from the neural network

    Returns:
        float: accuracy between the true data and the output of the neural network
    """
   
    # Find the expectation of relative error norm. 
    
    
    RL=0
    for i in range(true_Data.shape[0]):
        RL += (M_norm(true_Data[i]- nueral_network_output[i], true_Data[i]- nueral_network_output[i], M))/ (M_norm(true_Data[i], true_Data[i], M))
    Expecation_RL = RL/true_Data.shape[0]
    L2_error = 1- torch.sqrt(Expecation_RL)
    return L2_error

def H1_accuracy():
    print("H1")
    
def custom_loss_without_grad(output_PDE, outputs_NN, M=None):
    """
    Loss function without using the gradient information. 
    Loss = E[||output_PDE - output_NN||^2_2]
    Args:
        output_PDE (float): State solution of the PDE
        outputs_NN (float): Output generated by Neural Network

    Returns:
        float: Expected L2 square loss
    """
    
    N, D = outputs_NN.shape 
    del_u = output_PDE - outputs_NN
    # Step 1: reshape del_u to [N, 1, m] so we can use bmm
    del_u_reshaped = del_u.unsqueeze(1)
    # Step 2: compute del_u @ M => [N, 1, m] @ [m, m] = [N, 1, m]
    u_T_M = torch.matmul(del_u_reshaped, M)
    # Step 3: compute u.T M @ u => [N, 1, m] @ [N, m, 1] = [N, 1, 1]
    u = del_u.unsqueeze(2)  # [N, m, 1]
    result = torch.bmm(u_T_M, u)  # [N, 1, 1]
    result_batch = result.squeeze()
    
    #M2_norm = torch.einsum('ni,ij,nj->n', del_u, M, del_u)
    #l2_norm = torch.norm(output_PDE - outputs_NN, p=2, dim=1) ** 2  # Squared L2 norm for each sample
    #total_loss = torch.sum(l2_norm) /N #(N*D)  # Sum over all N samples (N *D)
    #total_loss = torch.sum(M2_norm) /N #(N*D)  # Sum over all N samples (N *D)
    
    total_loss = torch.mean(result_batch)
    return total_loss

def calulate_matrix_norm_square(output_grad_PDE, output_final, M):
    """
    Loss function between two Jacobain using matrix norm. 
    Loss = E[|| output_grad_PDE - output_final||_F^2 ]

    Args:
        output_grad_PDE (float): Sensitivity Jacobain, du/dk
        output_final (float): Jocobian matrix of Neural network output w.r.t to input

    Returns:
        float: Expectation of Matrix norm square
    """
    
    A = output_final - output_grad_PDE
    # A: [N, m, n]
    # M: [m, m]
    N, D, _ = A.shape
    # Step 1: Compute MA: [N, m, n]
    # MA = torch.matmul(M, A)  # M @ A_i for each batch
   
    # Step 2: Compute A^T @ MA: [N, n, n]
    # At_M_A = torch.matmul(A.transpose(1, 2), MA)
    
    # # Step 3: Take trace of each [n, n] matrix
    # weighted_fro_norm_sq = torch.einsum('bii->b', At_M_A) /N
    
    # M_batch = M.unsqueeze(0).expand(N, -1, -1)  # [N, m, m]
    # MA_ = torch.bmm(M_batch, A)                 # [N, m, n]
    # At_M_A_ = torch.bmm(A.transpose(1, 2), MA_)  # [N, n, n]
    # weighted_fro_norm_sq_ = torch.einsum('bii->b', At_M_A_) /N
    
    At_M_A_2 = torch.einsum('bmi, mk, bkj->bij', A, M , A)
    weighted_fro_norm_sq_2 = torch.einsum('bii->b', At_M_A_2) /N
    
    # frob_norm = (torch.norm(A, p='fro', dim=(1, 2)) ** 2) / N #(N * D * D)
    # print("Type", A.dtype, M.dtype)
    # print("difference",frob_norm - weighted_fro_norm_sq)
    # print("difference",frob_norm - weighted_fro_norm_sq_)
    # print("difference",frob_norm - weighted_fro_norm_sq_2)
    #return frob_norm
    return weighted_fro_norm_sq_2

def calculate_jacobian_full(inputs, outputs_NN):
    """
    Calculate the full Jacobian matrix of the output of the neural network, w.r.t input of the neural network

    Args:
        inputs (float): Input of the Neural Network
        outputs_NN (float): Output of the Neural Network

    Returns:
        float: Jacobian matrix
    """
    
    # Ensure input requires gradients
    outputs =  outputs_NN
    
    jacobian_list = []
    for i in range(outputs.shape[1]):  # Loop over each output dimension
        grad_outputs = torch.zeros_like(outputs)
        grad_outputs[:, i] = 1.0  # Compute gradient for one output at a time

        # Retain graph so it can be used for loss.backward()
        jacobian_row = torch.autograd.grad(outputs, inputs, grad_outputs=grad_outputs,
                                        retain_graph=True, create_graph=True)[0]

        jacobian_list.append(jacobian_row)
    
    jacobian_matrix = torch.stack(jacobian_list, dim=1)  # Shape: [batch_size, output_dim, input_dim]
    
    return jacobian_matrix

def calculate_jacobian(U_k, inputs, outputs_NN):
    """
    
    Calculate the Jacobian matrix of the transformed output of the neural network, w.r.t input of the neural network
    
    Args:
        U_k (float): A matrix formed by k random eigenvectors forms by POD of state variable of output of PDE
        inputs (float): Input of the Neural Network
        outputs_NN (float): Output of the Neural Network

    Returns:
        float: Jacobian matrix
    """
    
    
    # Ensure input requires gradients
    
    outputs = torch.einsum("ab, cb->ca", U_k, outputs_NN)
    
    #print(outputs.shape)
    jacobian_list = []
    for i in range(outputs.shape[1]):  # Loop over each output dimension
        grad_outputs = torch.zeros_like(outputs)
        grad_outputs[:, i] = 1.0  # Compute gradient for one output at a time

        # Retain graph so it can be used for loss.backward()
        jacobian_row = torch.autograd.grad(outputs, inputs, grad_outputs=grad_outputs,
                                        retain_graph=True, create_graph=True)[0]

        jacobian_list.append(jacobian_row)
    
    jacobian_matrix = torch.stack(jacobian_list, dim=1)  # Shape: [batch_size, output_dim, input_dim]
    
    return jacobian_matrix
    
class NN_setup():
    def __init__(self, Gempy_Inputs, PDE_outputs, Jacobian, Decoder_matrix, bias, Mass_Matrix):
        """
        Constructor Method for the NeuralNet class

        Args:
            layer_sizes (list of int): A list of integer representing the number of nodes in each layers.
        """
        super(NN_setup, self).__init__() # Initialialise the parent class
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        ###############################################################################
        # Seed the randomness 
        ###############################################################################
        seed = 42           
        torch.manual_seed(seed)
        # Ensure deterministic behavior
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
        self.Gempy_Inputs = torch.tensor(Gempy_Inputs, dtype=dtype, device=self.device)
        self.PDE_outputs = torch.tensor(PDE_outputs, dtype=dtype, device=self.device)
        self.Jacobian = torch.tensor(Jacobian, dtype=dtype, device=self.device)
        self.Decoder_matrix = torch.tensor(Decoder_matrix, dtype=dtype, device=self.device)
        self.bias = torch.tensor(bias, dtype=dtype, device=self.device)
        self.Mass_matrix = torch.tensor(Mass_Matrix, dtype=dtype, device=self.device)
        
        ######################################################################################
        # Create the Dataset for Neural network
        ######################################################################################
        dataset = TensorDataset(self.Gempy_Inputs, self.PDE_outputs, self.Jacobian)
        N, r, m = self.Jacobian.shape
    
        train_size = int(0.6 * N)
        valid_size = int(0.2 * N)
        test_size = N - train_size - valid_size

        # Randomly split dataset
        train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])
        
        # Create DataLoaders
        self.train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        self.valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle = False)
        self.test_loader = DataLoader(test_dataset, batch_size=test_size, shuffle=False)
        
        # Check sizes
        print(f"Train size: {len(train_dataset)},Valid size: {len(valid_dataset)},  Test size: {len(test_dataset)}")
        
    def train_network_M2(self, layer_sizes, num_epochs,  lr=0.002):
        """
        This function is used to train the neural network under various conditions. 

        Args:
            Gempy_Inputs (float): Input data to train the network
            PDE_outputs (float): True output data is the solution of the PDE
            Jacobian (float): Sensitivity of the state variable w.r.t input of the Gempy
            layer_sizes (list of int): Network architecture
            num_epochs (int): Number of epochs for the network training
            
        """
        ######################################################################################
        # Instantiate model
        ######################################################################################
            
        model = NeuralNet(layer_sizes=layer_sizes).to(dtype=dtype)
        model.to(self.device)
        
        optimizer = optim.Adam(model.parameters(), lr=lr)
        
        # -------------------------------
        # Training Loop with Loss Tracking
        # -------------------------------
        num_epochs = num_epochs
        train_losses = []
        val_losses = []
        
        for epoch in range(num_epochs):
            model.train()  # Set model to training mode
            epoch_train_loss = 0

            for inputs, output_PDE , output_grad_PDE in self.train_loader:
                inputs, output_PDE , output_grad_PDE = inputs.to(self.device), output_PDE.to(self.device) , output_grad_PDE.to(self.device)  # Ensure float32
                optimizer.zero_grad()
                outputs_NN = model(inputs)  # Forward pass
                outputs = torch.matmul(outputs_NN, self.Decoder_matrix.T ) + self.bias
                loss = custom_loss_without_grad(output_PDE, outputs, self.Mass_matrix) 
                loss.backward()  # Backpropagation
                optimizer.step()  # Update weights
                epoch_train_loss += loss.item()
            
            epoch_train_loss /= len(self.train_loader)
            train_losses.append(epoch_train_loss)
            
            # -------------------------------
            # Evaluate on Validation (Test) Set
            # -------------------------------
            model.eval()  # Set model to evaluation mode
            epoch_val_loss = 0

            with torch.no_grad():
                for inputs, output_PDE , output_grad_PDE in self.valid_loader:
                    inputs, output_PDE , output_grad_PDE = inputs.to(self.device), output_PDE.to(self.device) , output_grad_PDE.to(self.device)  # Ensure float32
                    outputs_NN = model(inputs)
                    outputs = torch.matmul(outputs_NN, self.Decoder_matrix.T ) + self.bias
                    loss = custom_loss_without_grad(output_PDE, outputs, self.Mass_matrix)
                    epoch_val_loss += loss.item()
                    
            # Compute average validation loss
            epoch_val_loss /= len(self.test_loader)
            val_losses.append(epoch_val_loss)

            # Print progress every 10 epochs
            if epoch % 10 == 0 :
                print(f"Epoch [{epoch}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}")
            elif epoch ==(num_epochs-1):
                print(f"Epoch [{epoch}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}")
            
            
        model.eval()  # Set model to evaluation mode
        
        for inputs, output_PDE , output_grad_PDE in self.test_loader:
            inputs, output_PDE , output_grad_PDE = inputs.to(self.device), output_PDE.to(self.device) , output_grad_PDE.to(self.device)  # Ensure float32
            inputs.requires_grad_(True)
            outputs_NN = model(inputs)
            outputs = torch.matmul(outputs_NN, self.Decoder_matrix.T ) + self.bias
            L_2_1 = L2_accuracy(true_Data=output_PDE , nueral_network_output=outputs, M=self.Mass_matrix)
            print("L2 accuracy without Jacobian: ",L_2_1)
            Jacobian_NN = calculate_jacobian_full(inputs, outputs_NN)
            Jacobian_output = torch.einsum("ca,nab->ncb",self.Decoder_matrix, Jacobian_NN )
            
            Frobenius_norm = calulate_matrix_norm_square(output_grad_PDE, Jacobian_output, self.Mass_matrix) 
            #print("|| delta J||_M^2",output_grad_PDE.shape[0] * Frobenius_norm)
            #true_matrix_norm = torch.norm(output_grad_PDE, p='fro', dim=(1, 2))**2
            A_T_M_A = torch.einsum("bmi, mk, bkj->bij", output_grad_PDE, self.Mass_matrix, output_grad_PDE)
            true_matrix_norm = torch.einsum("bii->b", A_T_M_A)
            #print("||J||_M^2", true_matrix_norm)
            # print("Ratio", torch.mean(torch.sqrt(output_grad_PDE.shape[0] * Frobenius_norm)/torch.sqrt(true_matrix_norm)))
            H1 = 1 - torch.sqrt(torch.sum((Frobenius_norm)/true_matrix_norm))
            print("H1 accuracy : ", H1)
            
            
        torch.save(model, "./saved_model/model_without_jacobian.pth")
        ################################################################################
        # Find the result just based on L2 norm of output of NN and and reduced basis
        # and respespect to Derivative
        # || q(k) - f_\theta(k)||_2^2 + || D(q(k)) - D(f_\theta(k))||_2^2
        ################################################################################
        # Instantiate model
    def train_network_Jacobian_full(self, layer_sizes, num_epochs, scale_factor=1, lr=0.002 ):
        
        model = NeuralNet(layer_sizes=layer_sizes).to(dtype=dtype)
        model.to(self.device)
        optimizer = optim.Adam(model.parameters(), lr=lr)
        train_losses = []
        val_losses = []
        L_2 = []
        F_2 = []
        a=scale_factor
        
        for epoch in range(num_epochs):
            model.train()  # Set model to training mode
            epoch_train_loss = 0
            epoch_L2_loss = 0
            epoch_F2_loss = 0

            for inputs, output_PDE , output_grad_PDE in self.train_loader:
                inputs, output_PDE , output_grad_PDE = inputs.to(self.device), output_PDE.to(self.device) , output_grad_PDE.to(self.device)  # Ensure float32
                optimizer.zero_grad()
                # 🔥 Ensure inputs track gradients before computing Jacobian
                inputs.requires_grad_(True)
                outputs_NN = model(inputs)  # Forward pass
                outputs = torch.matmul(outputs_NN, self.Decoder_matrix.T ) + self.bias
                L_2_loss = custom_loss_without_grad(output_PDE, outputs, M=self.Mass_matrix)/2
                Jacobian_NN = calculate_jacobian_full(inputs, outputs_NN)
                Jacobian_output = torch.einsum("ca,nab->ncb",self.Decoder_matrix, Jacobian_NN )
                Frobenius_norm = calulate_matrix_norm_square(output_grad_PDE, Jacobian_output, self.Mass_matrix) 
                F_2_loss = a * torch.sum(Frobenius_norm)/2
                loss = (L_2_loss +   F_2_loss)
                loss.backward()  # Backpropagation
                optimizer.step()  # Update weights
                epoch_train_loss += loss.item()
                epoch_L2_loss    += L_2_loss.item()
                epoch_F2_loss    += F_2_loss.item()
                
            # Compute average training loss
            
            epoch_train_loss /= len(self.train_loader)
            epoch_L2_loss  /= len(self.train_loader)
            epoch_F2_loss  /= len(self.train_loader)
            train_losses.append(epoch_train_loss)
            L_2.append(epoch_L2_loss)
            F_2.append(epoch_F2_loss)
            # -------------------------------
            # Step 5: Evaluate on Validation (Test) Set
            # -------------------------------
            model.eval()  # Set model to evaluation mode
            epoch_val_loss = 0

            #with torch.no_grad():
            for inputs, output_PDE , output_grad_PDE in self.valid_loader:
                inputs, output_PDE , output_grad_PDE = inputs.to(self.device), output_PDE.to(self.device) , output_grad_PDE.to(self.device)  # Ensure float32
                inputs.requires_grad_(True)
                outputs_NN = model(inputs)
                outputs = torch.matmul(outputs_NN, self.Decoder_matrix.T ) + self.bias
                L_2_loss = custom_loss_without_grad(output_PDE, outputs, M=self.Mass_matrix)/2
                Jacobian_NN = calculate_jacobian_full(inputs, outputs_NN)
                Jacobian_output = torch.einsum("ca,nab->ncb",self.Decoder_matrix, Jacobian_NN )
                Frobenius_norm = calulate_matrix_norm_square(output_grad_PDE, Jacobian_output, self.Mass_matrix) 
                F_2_loss = a * torch.sum(Frobenius_norm)/2
                loss = (L_2_loss +   F_2_loss)
                epoch_val_loss += loss.item()

            # Compute average validation loss
            epoch_val_loss /= len(self.test_loader)
            val_losses.append(epoch_val_loss)

            # Print progress every 10 epochs
            if epoch % 10 == 0 :
                print(f"Epoch [{epoch}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, L2 Loss: {epoch_L2_loss:.4f}, F2 Loss: {epoch_F2_loss:.4f}")
            elif epoch ==(num_epochs-1):
                print(f"Epoch [{epoch}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, L2 Loss: {epoch_L2_loss:.4f}, F2 Loss: {epoch_F2_loss:.4f}")
        
        # -------------------------------
        # Test data: Find accuracies
        # -------------------------------
        model.eval()  # Set model to evaluation mode
    
        for inputs, output_PDE , output_grad_PDE in self.test_loader:
            inputs, output_PDE , output_grad_PDE = inputs.to(self.device), output_PDE.to(self.device) , output_grad_PDE.to(self.device)  # Ensure float32
            inputs.requires_grad_(True)
        
            outputs_NN = model(inputs)
            outputs = torch.matmul(outputs_NN, self.Decoder_matrix.T) + self.bias
            
            L_2 = L2_accuracy(true_Data=output_PDE , nueral_network_output=outputs, M=self.Mass_matrix)
            print("L2 accuracy with Jacobian: ",L_2)
            Jacobian_NN = calculate_jacobian_full(inputs, outputs_NN)
            Jacobian_output = torch.einsum("ca,nab->ncb",self.Decoder_matrix, Jacobian_NN )
            
            Frobenius_norm = calulate_matrix_norm_square(output_grad_PDE, Jacobian_output, self.Mass_matrix) 
            #print("|| delta J||_M^2",output_grad_PDE.shape[0] * Frobenius_norm)
            #true_matrix_norm = torch.norm(output_grad_PDE, p='fro', dim=(1, 2))**2
            A_T_M_A = torch.einsum("bmi, mk, bkj->bij", output_grad_PDE, self.Mass_matrix, output_grad_PDE)
            true_matrix_norm = torch.einsum("bii->b", A_T_M_A)
            #print("||J||_M^2", true_matrix_norm)
            #print("Ratio", torch.mean(torch.sqrt(output_grad_PDE.shape[0] * Frobenius_norm)/torch.sqrt(true_matrix_norm)))
            H1 = 1 - torch.sqrt(torch.sum(Frobenius_norm/true_matrix_norm))
            print("H1 accuracy : ", H1)
            
        torch.save(model, "./saved_model/model_jacobian_full.pth")
        
    def train_network_Jacobian_truncated(self, layer_sizes, num_epochs, scale_factor=1, ran_size=2):   
        ######################################################################################
        # Find U and V by taking expextation of Jacobian
        ######################################################################################
        N, r, m = self.Jacobian.shape
        result_grad_sum_left = torch.zeros(r,r, dtype=dtype, device=self.device)  # Initialize a tensor to accumulate the sum
        result_grad_sum_right = torch.zeros(m,m, dtype=dtype, device=self.device)
        
        for i in range(N):
            grad_u = self.Jacobian[i]
            result_grad_sum_left += grad_u @ grad_u.T
            result_grad_sum_right += grad_u.T @ grad_u
        # Compute the average

        cov_matrix_grad_m_u_left = result_grad_sum_left / N
        cov_matrix_grad_m_u_right = result_grad_sum_right / N
        
        # find the eigen_values of u 
        _, U = torch.linalg.eig(cov_matrix_grad_m_u_left.detach())
        _, V = torch.linalg.eig(cov_matrix_grad_m_u_right.detach())
        
        U = U.real
        V = V.real

        
            
        model = NeuralNet(layer_sizes=layer_sizes)
        model.to(self.device)
        
        optimizer = optim.Adam(model.parameters(), lr=0.002)
        train_losses = []
        val_losses = []
        L_2 = []
        F_2 = []
        a=scale_factor
        ran_size = int(ran_size)
        
        ran_iter = 5 * int(U.shape[0]/ran_size)
        
        for epoch in range(num_epochs):
            model.train()  # Set model to training mode
            epoch_train_loss = 0
            epoch_L2_loss = 0
            epoch_F2_loss = 0

            for inputs, output_PDE , output_grad_PDE in self.train_loader:
                inputs, output_PDE , output_grad_PDE = inputs.float().to(self.device), output_PDE.to(self.device) , output_grad_PDE.to(self.device)  # Ensure float32
                optimizer.zero_grad()
                # 🔥 Ensure inputs track gradients before computing Jacobian
                inputs.requires_grad_(True)
                outputs_NN = model(inputs)  # Forward pass
                outputs = torch.matmul(outputs_NN, self.Decoder_matrix.T ) + self.bias
                norm = 0
                V_k_tilde = V
                for i in range(ran_iter):
                    # Draw k numbers uniformly from {1, 2, ..., r}
                    k = torch.randperm(r)[:ran_size] #+ 1  # Add 1 to shift range from [0, r-1] to [1, r]
                    U_k = U[:,k]
                    U_k_phi = U_k.T @ self.Decoder_matrix
                    U_k_T_nabla_q = torch.matmul(U_k.T, output_grad_PDE)
                    grad_U_k_phi_Output_NN = calculate_jacobian(U_k_phi , inputs, outputs_NN)
                    norm += calulate_matrix_norm_square(U_k_T_nabla_q, grad_U_k_phi_Output_NN) 
                matrix_norm_expectation = (norm * r )/ (k.shape[0] * ran_iter)
                L_2_loss = custom_loss_without_grad(output_PDE, outputs)/2
                F_2_loss = a * torch.sum(matrix_norm_expectation)/2
                loss = (L_2_loss +   F_2_loss)
                loss.backward()  # Backpropagation
                optimizer.step()  # Update weights
                epoch_train_loss += loss.item()
                epoch_L2_loss    += L_2_loss.item()
                epoch_F2_loss    += F_2_loss.item()
                
            # Compute average training loss
            
            epoch_train_loss /= len(self.train_loader)
            epoch_L2_loss  /= len(self.train_loader)
            epoch_F2_loss  /= len(self.train_loader)
            train_losses.append(epoch_train_loss)
            L_2.append(epoch_L2_loss)
            F_2.append(epoch_F2_loss)
            # -------------------------------
            # Step 5: Evaluate on Validation (Test) Set
            # -------------------------------
            model.eval()  # Set model to evaluation mode
            epoch_val_loss = 0

            #with torch.no_grad():
            for inputs, output_PDE , output_grad_PDE in self.valid_loader:
                inputs, output_PDE , output_grad_PDE = inputs.float().to(self.device), output_PDE.to(self.device) , output_grad_PDE.to(self.device)  # Ensure float32
                inputs.requires_grad_(True)
                outputs_NN = model(inputs)
                outputs = torch.matmul(outputs_NN, self.Decoder_matrix.T ) + self.bias
                norm = 0
                V_k_tilde = V
                for i in range(ran_iter):
                    # Draw k numbers uniformly from {1, 2, ..., r}
                    k = torch.randperm(r)[:ran_size] #+ 1  # Add 1 to shift range from [0, r-1] to [1, r]
                    U_k = U[:,k]
                    U_k_phi = U_k.T @ self.Decoder_matrix
                    U_k_T_nabla_q = torch.matmul(U_k.T, output_grad_PDE)
                    grad_U_k_phi_Output_NN = calculate_jacobian(U_k_phi , inputs, outputs_NN)
                    norm += calulate_matrix_norm_square(U_k_T_nabla_q, grad_U_k_phi_Output_NN) 
                    
                matrix_norm_expectation = (norm * r )/ (k.shape[0] * ran_iter)
                L_2_loss = custom_loss_without_grad(output_PDE, outputs)/2
                F_2_loss = a * torch.sum(matrix_norm_expectation)/2
                loss = (L_2_loss +  F_2_loss)
                epoch_val_loss += loss.item()
                

            # Compute average validation loss
            epoch_val_loss /= len(self.test_loader)
            val_losses.append(epoch_val_loss)

            
            if epoch % 10 == 0 :
                print(f"Epoch [{epoch}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, L2 Loss: {epoch_L2_loss:.4f}, F2 Loss: {epoch_F2_loss:.4f}")
            elif epoch ==(num_epochs-1):
                print(f"Epoch [{epoch}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, L2 Loss: {epoch_L2_loss:.4f}, F2 Loss: {epoch_F2_loss:.4f}")
                
            model.eval()  # Set model to evaluation mode
    
        for inputs, output_PDE , output_grad_PDE in self.test_loader:
            inputs, output_PDE , output_grad_PDE = inputs.float().to(self.device), output_PDE.to(self.device) , output_grad_PDE.to(self.device)  # Ensure float32
            inputs.requires_grad_(True)
            outputs_NN = model(inputs)
            outputs = torch.matmul(outputs_NN, self.Decoder_matrix.T ) + self.bias
            L_2 = L2_accuracy(true_Data=output_PDE , nueral_network_output=outputs)
            print("L2 accuracy with Jacobian: ",L_2)
            norm = 0
            for i in range(ran_iter):
                # Draw k numbers uniformly from {1, 2, ..., r}
                k = torch.randperm(r)[:ran_size] #+ 1  # Add 1 to shift range from [0, r-1] to [1, r]
                U_k = U[:,k]
                U_k_phi = U_k.T @ self.Decoder_matrix
                U_k_T_nabla_q = torch.matmul(U_k.T, output_grad_PDE)
                grad_U_k_phi_Output_NN = calculate_jacobian(U_k_phi , inputs, outputs_NN)
                norm += calulate_matrix_norm_square(U_k_T_nabla_q, grad_U_k_phi_Output_NN) 
            matrix_norm_expectation = (norm * r )/ (k.shape[0] * ran_iter)
            true_matrix_norm = torch.norm(output_grad_PDE, p='fro', dim=(1, 2))**2
            H1 = 1- torch.sqrt(torch.mean((matrix_norm_expectation)/true_matrix_norm))
            print("H1 accuracy : ", H1)
            
        torch.save(model, "./saved_model/model_jacobian_truncated.pth")
            